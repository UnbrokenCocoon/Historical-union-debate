{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iWpaR_ZkHBVG"
      },
      "outputs": [],
      "source": [
        "!pip install faiss-cpu langchain langchain-community hf_xet langchain-huggingface langchain_google_genai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import shutil\n",
        "# Detect if running in Google Colab\n",
        "\n",
        "# Set the environment variable for your GitHub token\n",
        "#os.environ[\"GITHUB_TOKEN\"] =\n",
        "\n",
        "# This cell is for loading data. If your prefer to do this manually, you will need to set base_dir and data_dir separately\n",
        "\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "# Check if running in Google Colab\n",
        "if IN_COLAB:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    # Set the base directory on Google Drive (no extra folder will be added)\n",
        "    base_dir = \"/content/drive/MyDrive/Bertopic\"\n",
        "    token = os.getenv(\"GITHUB_TOKEN\")\n",
        "    #if os.path.exists(base_dir):\n",
        "     #   shutil.rmtree(base_dir)\n",
        "\n",
        "    #!git clone https://{token}@github.com/UnbrokenCocoon/OCR-evaluation.git \"{base_dir}\"\n",
        "\n",
        "else:\n",
        "    # Set the base directory locally (set this to your local project folder)\n",
        "    base_dir = \"path/to/your/local/project/folder\"\n",
        "\n",
        "    #!git clone https://{token}@github.com/UnbrokenCocoon/OCR-evaluation.git \"{base_dir}\"\n",
        "\n",
        "    # Clone the repository locally\n",
        "\n",
        "\n",
        "# Set the data directory (this assumes you have a 'Data' folder inside the repository)\n",
        "data_dir = os.path.join(base_dir, \"Data\")\n",
        "output_dir = os.path.join(base_dir, \"output\")\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Now data_dir points to the cloned Data folder\n",
        "print(f\"Data folder is located at: {data_dir}\")\n"
      ],
      "metadata": {
        "id": "eCWrDSMgSQYT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eo0WwYfrDz7y"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "with open(os.path.join(data_dir, 'bs_emb.pkl', 'rb') as f:\n",
        "    bs_emb = pickle.load(f)\n",
        "with open(os.path.join(data_dir, 'bs_sen.pkl', 'rb') as f:\n",
        "    bs_sen = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_iYUlFRjFVUD"
      },
      "outputs": [],
      "source": [
        "with open(os.path.join(data_dir, 'uc_sentences.pkl', 'rb') as f:\n",
        "  uc_sen = pickle.load(f)\n",
        "with open(os.path.join(data_dir, 'uc_embedding.pkl', 'rb') as f:\n",
        "  uc_emb = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IfTis1Y2LMO0"
      },
      "outputs": [],
      "source": [
        "# The sentences are wrapped as docs for reliable processing\n",
        "# A more simplistic approach could be used, but this was found to cause errors sometimes\n",
        "import faiss\n",
        "import os\n",
        "import numpy as np\n",
        "from langchain.docstore import InMemoryDocstore\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain.schema import Document\n",
        "\n",
        "faiss_dir = os.path.join(data_dir, \"faiss\")\n",
        "os.makedirs(faiss_dir, exist_ok=True)\n",
        "\n",
        "# Ensure float32 for FAISS\n",
        "embeddings = np.array(bs_emb).astype(\"float32\")\n",
        "\n",
        "# Step 1: Build FAISS index\n",
        "dimension = embeddings.shape[1]\n",
        "index = faiss.IndexFlatL2(dimension)\n",
        "index.add(embeddings)\n",
        "\n",
        "# Step 2: Wrap sentences as Documents\n",
        "docs = [Document(page_content=txt) for txt in bs_sen]\n",
        "docstore = InMemoryDocstore(dict(enumerate(docs)))\n",
        "index_to_docstore_id = dict(enumerate(range(len(docs))))\n",
        "\n",
        "# Step 3: Set up embedding model again for retrieval later\n",
        "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
        "\n",
        "# Step 4: Create LangChain FAISS object\n",
        "faiss_store = FAISS(\n",
        "    embedding_function=embedding_model,\n",
        "    index=index,\n",
        "    docstore=docstore,\n",
        "    index_to_docstore_id=index_to_docstore_id\n",
        ")\n",
        "\n",
        "# Step 5: Save\n",
        "faiss_store.save_local(os.path.join(faiss_dir, \"index_a\"))\n",
        "print(\"‚úÖ Successfully built and saved FAISS index_a.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2RGJZ9kFLUQ6"
      },
      "outputs": [],
      "source": [
        "import faiss\n",
        "import numpy as np\n",
        "from langchain.docstore import InMemoryDocstore\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain.schema import Document\n",
        "\n",
        "# Ensure float32 for FAISS\n",
        "embeddings = np.array(uc_emb).astype(\"float32\")\n",
        "\n",
        "# Step 1: Build FAISS index\n",
        "dimension = embeddings.shape[1]\n",
        "index = faiss.IndexFlatL2(dimension)\n",
        "index.add(embeddings)\n",
        "\n",
        "# Step 2: Wrap sentences as Documents\n",
        "docs = [Document(page_content=txt) for txt in uc_sen]\n",
        "docstore = InMemoryDocstore(dict(enumerate(docs)))\n",
        "index_to_docstore_id = dict(enumerate(range(len(docs))))\n",
        "\n",
        "# Step 3: Set up embedding model again for retrieval later\n",
        "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
        "\n",
        "# Step 4: Create LangChain FAISS object\n",
        "faiss_store = FAISS(\n",
        "    embedding_function=embedding_model,\n",
        "    index=index,\n",
        "    docstore=docstore,\n",
        "    index_to_docstore_id=index_to_docstore_id\n",
        ")\n",
        "\n",
        "# Step 5: Save\n",
        "faiss_store.save_local(os.path.join(faiss_dir, \"index_b\"))\n",
        "print(\"‚úÖ Successfully built and saved FAISS index_b.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qkNNVlwKG9EX"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "faiss_dir = os.path.join(data_dir, \"faiss\")\n",
        "# Load embedding model\n",
        "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
        "\n",
        "# Load both FAISS indexes\n",
        "faiss_a = FAISS.load_local(os.path.join(faiss_dir, \"index_a\"), embeddings=embedding_model, allow_dangerous_deserialization=True)\n",
        "faiss_b = FAISS.load_local(os.path.join(faiss_dir, \"index_b\"), embeddings=embedding_model, allow_dangerous_deserialization=True)\n",
        "\n",
        "retriever_a = faiss_a.as_retriever(search_kwargs={\"k\": 20})\n",
        "retriever_b = faiss_b.as_retriever(search_kwargs={\"k\": 20})\n",
        "\n",
        "# Sample query\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1q08mcU-LBPi"
      },
      "outputs": [],
      "source": [
        "# Test the retrieval\n",
        "query = \"the film the big\"\n",
        "retriever_a = faiss_a.as_retriever(search_kwargs={\"k\": 50})\n",
        "retriever_b = faiss_b.as_retriever(search_kwargs={\"k\": 50})\n",
        "# Retrieve and print results\n",
        "print(\"\\nüî¥ Top 5 from index_a (Boot & Shoe):\")\n",
        "for i, doc in enumerate(retriever_a.get_relevant_documents(query)):\n",
        "    print(f\"{i+1}. {doc.page_content}\")\n",
        "\n",
        "print(\"\\nüîµ Top 5 from index_b (Unite Community):\")\n",
        "for i, doc in enumerate(retriever_b.get_relevant_documents(query)):\n",
        "    print(f\"{i+1}. {doc.page_content}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "88h0YRrpO6dD"
      },
      "outputs": [],
      "source": [
        "api_key = #set your API key\n",
        "#Test it works\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(\n",
        "    api_key=api_key,\n",
        "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
        ")\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gemini-2.0-flash\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Explain to me how AI works\"\n",
        "        }\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(response.choices[0].message)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vEhXkItyXxnO"
      },
      "outputs": [],
      "source": [
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.schema import HumanMessage, SystemMessage\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# üîß Set your Gemini API key\n",
        "os.environ[\"GOOGLE_API_KEY\"] = api_key  # Make sure `api_key` is defined\n",
        "\n",
        "# üîç Load the embedding model\n",
        "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
        "\n",
        "dialogue_history = []\n",
        "\n",
        "# üîÅ Set up retrievers\n",
        "retriever_a = faiss_a.as_retriever(search_kwargs={\"k\": 10})\n",
        "retriever_b = faiss_b.as_retriever(search_kwargs={\"k\": 10})\n",
        "\n",
        "# üß† System prompts\n",
        "system_prompt_a = \"\"\"You are a 1920s trade union representative from the National Boot and Shoe Union.\n",
        "Use the retrieved sentences as your knowledge base.\n",
        "Speak persuasively as if you are arguing with a fellow trade unionist.\n",
        "Do not use your own knowledge.\n",
        "Vary your sentence structures, do not repeat phrases.\n",
        "Respond with 2 sentences maximum.\"\"\"\n",
        "\n",
        "system_prompt_b = \"\"\"You are a 2020s trade union representative from Unite Community.\n",
        "Use the retrieved sentences as your knowledge base.\n",
        "Speak persuasively as if you are arguing with a fellow trade unionist.\n",
        "Do not use your own knowledge.\n",
        "Vary your sentence structures, do not repeat phrases.\n",
        "Respond with 2 sentences maximum.\"\"\"\n",
        "\n",
        "# üìÉ Store dialogue\n",
        "\n",
        "# üîÑ Dialogue generator\n",
        "def generate_turn(query, retriever, system_prompt, speaker):\n",
        "    docs = retriever.get_relevant_documents(query)\n",
        "    content = \"\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "    prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", system_prompt),\n",
        "        (\"human\", f\"You just heard the following message:\\n\\n\\\"{query}\\\"\\n\\nHere are 5 excerpts from your documents that may help you reply:\\n{content}\\n\\nRespond to the message above based ONLY on this information, and speak as if you were in a real conversation.\")\n",
        "    ])\n",
        "    response = llm(prompt.format_messages())\n",
        "    reply_text = response.content.strip().replace(\"\\n\", \" \")\n",
        "    print(f\"\\n{speaker}:\\n{reply_text}\\n{'-'*50}\")\n",
        "    dialogue_history.append({\n",
        "    \"speaker\": speaker,\n",
        "    \"query\": query,\n",
        "    \"response\": reply_text,\n",
        "    \"context\": content\n",
        "    })\n",
        "    return reply_text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "39KHH5tKT6b3"
      },
      "outputs": [],
      "source": [
        "# You may want to reload, it is here so it writes over the init of the variable\n",
        "with open(os.path.join(data_dir, 'dialogue_history.pkl'), 'rb') as f:\n",
        "  dialogue_history=pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p3G7DvlTeLgZ"
      },
      "outputs": [],
      "source": [
        "# Check the len and split to a multiple of 5 if any calls fail\n",
        "print(len(dialogue_history))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "UzfvO13vlhFS"
      },
      "outputs": [],
      "source": [
        "models = [\"gemini-2.0-flash-lite\", \"gemini-2.0-flash\", \"gemini-2.0-flash-lite\", \"gemini-2.0-flash\", \"gemini-2.0-flash-lite\"]\n",
        "# üó£Ô∏è Start the dialogue with high call freq models\n",
        "import random\n",
        "import time\n",
        "for j in range(40):\n",
        "  time.sleep(20)\n",
        "  sentence_index = range(0, len(bs_sen),1)\n",
        "  query = bs_sen[n]\n",
        "  for i in range(5):\n",
        "      llm = ChatGoogleGenerativeAI(model=models[i], temperature=0.7)\n",
        "      if i % 2 == 0:\n",
        "          n+=1\n",
        "          speaker = \"üî¥ Boot & Shoe Union (1920s)\"\n",
        "          query = generate_turn(query, retriever_a, system_prompt_a, speaker)\n",
        "      else:\n",
        "          n+=1\n",
        "          speaker = \"üîµ Unite Community (2020s)\"\n",
        "          query = generate_turn(query, retriever_b, system_prompt_b, speaker)\n",
        "with open(os.path.join(data_dir, 'dialogue_history.pkl'), 'wb') as f:\n",
        "  pickle.dump(dialogue_history, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HVSotMdodRrk"
      },
      "outputs": [],
      "source": [
        "# üó£Ô∏è Start the dialogue with low call models\n",
        "import random\n",
        "import time\n",
        "list_of_models = [\"gemini-1.5-flash\", \"gemini-1.5-flash-8b\", \"gemini-2.0-flash-lite\", \"gemini-2.0-flash\", \"gemini-1.5-flash-8b\"]\n",
        "for j in range(40):\n",
        "  time.sleep(20)\n",
        "  sentence_index = range(0, len(bs_sen),1)\n",
        "  query = bs_sen[random.choice(sentence_index)]\n",
        "  for i in range(5):\n",
        "      llm = ChatGoogleGenerativeAI(model=list_of_models[i], temperature=0.7)\n",
        "      if i % 2 == 0:\n",
        "          speaker = \"üî¥ Boot & Shoe Union (1920s)\"\n",
        "          query = generate_turn(query, retriever_a, system_prompt_a, speaker)\n",
        "      else:\n",
        "          speaker = \"üîµ Unite Community (2020s)\"\n",
        "          query = generate_turn(query, retriever_b, system_prompt_b, speaker)\n",
        "with open(os.path.join(data_dir, 'dialogue_history.pkl'), 'wb') as f:\n",
        "  pickle.dump(dialogue_history, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iZnPkOhVezKU"
      },
      "outputs": [],
      "source": [
        "print(dialogue_df.head(5).to_markdown())"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}